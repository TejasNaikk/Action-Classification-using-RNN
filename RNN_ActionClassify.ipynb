{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_ActionClassify.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "2a75xZqR9x91",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sequence Classification using Recurrent Neural Networks(RNN)\n",
        "In this homework, you will learn how to train a recurrent neural network for human action classification. RNN is designed handle sequential data. The network can incorporate both past history and current input. [This](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) is a very good tutorial. You should read it before you start."
      ]
    },
    {
      "metadata": {
        "id": "lxpcn6zf9x95",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "**Please make sure you have h5py and torchnet installed**\n",
        "> pip install h5py\n",
        "\n",
        "> pip install git+https://github.com/pytorch/tnt.git@master\n",
        "\n",
        "## Known Windows Issues:\n",
        "### In case you're getting an error [Read more](https://discuss.pytorch.org/t/brokenpipeerror-errno-32-broken-pipe-when-i-run-cifar10-tutorial-py/6224): \n",
        "```python\n",
        "BrokenPipeError: [Errno 32] Broken pipe\n",
        "```\n",
        "\n",
        ">In the dataloader block change Line 39, 42, and 45 num_workers=0 \n",
        "\n",
        "### In case of error (This should be a CUDA error [Read more](https://discuss.pytorch.org/t/asserterror-in-lstm-layer-on-gpu/8698)):\n",
        "\n",
        "```python\n",
        "--> 186             assert param_from.type() == param_to.type()\n",
        "AssertionError: \n",
        "```\n",
        "\n",
        "**Replace following lines:**\n",
        "```python\n",
        "def run_epoch(data_loader, model, criterion, epoch, is_training, optimizer=None):\n",
        "    ...\n",
        "    input_sequence_var = Variable(sequence).type(FloatTensor)\n",
        "    input_label_var = Variable(label).type(LongTensor)\n",
        "    ...\n",
        "```\n",
        "```python\n",
        "def predict_on_test(model, data_loader):\n",
        "    ...\n",
        "        input_sequence_var = Variable(sequence).type(FloatTensor)\n",
        "    ...\n",
        "```\n",
        "** With: **\n",
        "```python\n",
        "def run_epoch(data_loader, model, criterion, epoch, is_training, optimizer=None):\n",
        "    ...\n",
        "    input_sequence_var = Variable(sequence)\n",
        "    input_label_var = Variable(label)\n",
        "    ...\n",
        "```\n",
        "```python\n",
        "def predict_on_test(model, data_loader):\n",
        "    ...\n",
        "        input_sequence_var = Variable(sequence)\n",
        "    ...\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "RaKS_nDA-NOF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/pytorch/tnt.git@master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kpxvaEep9x96",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "00e17bb6-2165-46ce-f3c9-b886f68564db",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525559376562,
          "user_tz": 240,
          "elapsed": 362,
          "user": {
            "displayName": "Tejas Naik",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "109987961230950503945"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as DD\n",
        "import torchnet as tnt\n",
        "\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('use cuda: %s'%(use_cuda))\n",
        "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
        "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "use cuda: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5gGbJrKe_aNh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rMzEZ4IX_1x9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UVvT9-0rAGWt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4b77f60-ec15-4775-a631-6b9db1fe1bdd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525554251404,
          "user_tz": 240,
          "elapsed": 1160,
          "user": {
            "displayName": "Tejas Naik",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "109987961230950503945"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "fileId = drive.CreateFile({'id': '14Jx-VPNPx1_o8J3qBRSKWDSNN2QAmBlB'}) #DRIVE_FILE_ID is file id example: 1iytA1n2z4go3uVCwE_vIKouTKyIDjEq\n",
        "print fileId['title'] \n",
        "fileId.GetContentFile('hw5.zip')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hw5.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8drHkJmAC8U-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "a73ae5bd-794f-4138-d64b-0913a973bed9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525554254328,
          "user_tz": 240,
          "elapsed": 1557,
          "user": {
            "displayName": "Tejas Naik",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "109987961230950503945"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip hw5.zip -d ./"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  hw5.zip\r\n",
            "   creating: ./hw5/\r\n",
            "  inflating: ./hw5/.DS_Store         \r\n",
            "   creating: ./__MACOSX/\r\n",
            "   creating: ./__MACOSX/hw5/\r\n",
            "  inflating: ./__MACOSX/hw5/._.DS_Store  \r\n",
            "  inflating: ./hw5/RNN_ActionClassify.ipynb  \r\n",
            "  inflating: ./__MACOSX/hw5/._RNN_ActionClassify.ipynb  \r\n",
            "   creating: ./hw5/data/\r\n",
            "  inflating: ./hw5/data/val_label.h5  \r\n",
            "   creating: ./__MACOSX/hw5/data/\r\n",
            "  inflating: ./__MACOSX/hw5/data/._val_label.h5  \r\n",
            "  inflating: ./hw5/data/.DS_Store    \r\n",
            "  inflating: ./__MACOSX/hw5/data/._.DS_Store  \r\n",
            "  inflating: ./hw5/data/train_label.h5  \r\n",
            "  inflating: ./__MACOSX/hw5/data/._train_label.h5  \n",
            "  inflating: ./hw5/data/val_data.h5  \n",
            "  inflating: ./__MACOSX/hw5/data/._val_data.h5  \n",
            "  inflating: ./hw5/data/test_data.h5  \n",
            "  inflating: ./__MACOSX/hw5/data/._test_data.h5  \n",
            "  inflating: ./hw5/data/train_data.h5  \n",
            "  inflating: ./__MACOSX/hw5/data/._train_data.h5  \n",
            "  inflating: ./__MACOSX/hw5/._data   \n",
            "  inflating: ./__MACOSX/._hw5        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5hLCRhy09x-E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "The data we are using is skeleton data, which indicates the 3D locations of body joints. In total, there are 25 body joints. It is collected by Kinect v2. To make it easier, each sequence have same number of frames. You need to classify 10 different actions. There are 4000 training sequences, 800 validation sequences, and 1000 test sequences. Each sequence has 15 frames, each frame is a 75-dimension vector (3*25).\n",
        "\n",
        "For your convenience, we provide the dataloader for you.\n"
      ]
    },
    {
      "metadata": {
        "id": "XrFuOmdl9x-F",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Dataset(DD.Dataset):\n",
        "    # subset can be: 'train', 'val', 'test'\n",
        "    def __init__(self, data_path, subset='train'):\n",
        "        super(Dataset, self).__init__()\n",
        "        self.data_path = os.path.join(data_path, '%s_data.h5'%subset)\n",
        "        self.subset = subset\n",
        "\n",
        "        with h5py.File(self.data_path) as f:\n",
        "            self.data = np.array(f['data'])\n",
        "\n",
        "        if subset != 'test':\n",
        "            self.label_path = os.path.join(data_path, '%s_label.h5'%subset)\n",
        "            with h5py.File(self.label_path) as f:\n",
        "                self.label = np.array(f['label'])\n",
        "\n",
        "        self.num_sequences = self.data.shape[0]\n",
        "        self.seq_len = self.data.shape[1]\n",
        "        self.n_dim = self.data.shape[2]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        seq = self.data[index]\n",
        "        if self.subset != 'test':\n",
        "            label = int(self.label[index])\n",
        "            sample = {'seq': seq, 'label': label}\n",
        "        else:\n",
        "            sample = {'seq': seq}\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_sequences\n",
        "\n",
        "trSet = Dataset('./hw5/data', subset='train')\n",
        "valSet = Dataset('./hw5/data', subset='val')\n",
        "tstSet = Dataset('./hw5/data', subset='test')\n",
        "\n",
        "batch_size = 50\n",
        "trLD = DD.DataLoader(trSet, batch_size=batch_size,\n",
        "       sampler=DD.sampler.RandomSampler(trSet),\n",
        "       num_workers=2, pin_memory=False)\n",
        "valLD = DD.DataLoader(valSet, batch_size=batch_size,\n",
        "       sampler=DD.sampler.SequentialSampler(valSet),\n",
        "       num_workers=1, pin_memory=False)\n",
        "tstLD = DD.DataLoader(tstSet, batch_size=batch_size,\n",
        "       sampler=DD.sampler.SequentialSampler(tstSet),\n",
        "       num_workers=1, pin_memory=False)\n",
        "\n",
        "input_dim = trSet.n_dim\n",
        "num_class = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IYjEuie8zpVc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "OrderedDict = collections.OrderedDict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ACWrh3nK9x-H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "Pytorch has implemented different types of recurrent layers for you. For this homework, you can use any type of RNNs as you want:\n",
        "> torch.nn.RNN()\n",
        "\n",
        "> torch.nn.LSTM()\n",
        "\n",
        "> torch.nn.GRU()\n",
        "\n",
        "You can check details for different types of recurrent layers here: [RNN](http://pytorch.org/docs/master/nn.html#torch.nn.RNN), [LSTM]( http://pytorch.org/docs/master/nn.html#torch.nn.LSTM), [GRU](http://pytorch.org/docs/master/nn.html#torch.nn.GRU)\n",
        "\n",
        "\n",
        "### Implement a specific model\n",
        "In this section, you need to implement a model for sequence classification. The model has following layers:\n",
        "* 1 Layer LSTM layer with hidden size of 100, and input size of 75\n",
        "* A linear layer that goes from 100 to num_class (10). \n",
        "\n",
        "An LSTM layer takes an input of size of (batch_size, seq_len, fea_dim) and outputs a variable of shape (batch_size, seq_len, hidden_size). In this homework, the classification score for a sequence is the classification score for the last step of rnn_outputs.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "j8S1_wfC9x-I",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# sequence classification model\n",
        "class SequenceClassify(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SequenceClassify, self).__init__()\n",
        "        \n",
        "        ############## 1st To Do (20 points) ##############\n",
        "        ###################################################\n",
        "             \n",
        "        self.recurrent_layer = nn.LSTM(75,120,2,batch_first=True,dropout=0.3)\n",
        "        self.Relu=nn.ReLU()\n",
        "        self.project_layer = nn.Linear(120,10)\n",
        "        ###################################################\n",
        "    \n",
        "    # the size of input is [batch_size, seq_len(15), input_dim(75)]\n",
        "    # the size of logits is [batch_size, num_class]\n",
        "    def forward(self, input, h_t_1=None, c_t_1=None):\n",
        "        # the size of rnn_outputs is [batch_size, seq_len, rnn_size]\n",
        "        rnn_outputs, (hn, cn) = self.recurrent_layer(input)\n",
        "        # classify the last step of rnn_outpus\n",
        "        # the size of logits is [batch_size, num_class]\n",
        "        rnn_outputs=self.Relu(rnn_outputs)\n",
        "        logits = self.project_layer(rnn_outputs[:,-1])\n",
        "        logits=self.Relu(rnn_outputs)\n",
        "        logits = self.project_layer(rnn_outputs[:,-1])\n",
        "        return logits\n",
        "\n",
        "model = SequenceClassify()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Epr0Wyp9zdJt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jQymNgUz9x-K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "After you have the dataloader and model, you can start training the model. Define a SGD optimizer with learning rate of 1e-2, and a cross-entropy loss function:"
      ]
    },
    {
      "metadata": {
        "id": "DM4KvFHA9x-L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "################ 2nd To Do  (5 points)##################\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 1e-1)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z4oqpCNb9x-N",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 6329
        },
        "outputId": "b190b0f6-da59-4ea5-8713-538e111f6ef6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525574498552,
          "user_tz": 240,
          "elapsed": 1878956,
          "user": {
            "displayName": "Tejas Naik",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "109987961230950503945"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# run the model for one epoch\n",
        "# can be used for both training or validation model\n",
        "def run_epoch(data_loader, model, criterion, epoch, is_training, optimizer=None):\n",
        "    if is_training:\n",
        "        model.train()\n",
        "        logger_prefix = 'train'\n",
        "    else:\n",
        "        model.eval()\n",
        "        logger_prefix = 'val'\n",
        "\n",
        "    confusion_matrix = tnt.meter.ConfusionMeter(num_class)\n",
        "    acc = tnt.meter.ClassErrorMeter(accuracy=True)\n",
        "    meter_loss = tnt.meter.AverageValueMeter()\n",
        "\n",
        "    for batch_idx, sample in enumerate(data_loader):\n",
        "        sequence = sample['seq']\n",
        "        label = sample['label']\n",
        "        input_sequence_var = Variable(sequence)  \n",
        "        input_label_var = Variable(label)\n",
        "        \n",
        "\n",
        "        # compute output\n",
        "        # output_logits: [batch_size, num_class]\n",
        "        output_logits = model(input_sequence_var)\n",
        "        loss = criterion(output_logits, input_label_var)\n",
        "\n",
        "        if is_training:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        meter_loss.add(loss.data[0])\n",
        "        acc.add(output_logits.data, input_label_var.data)\n",
        "        confusion_matrix.add(output_logits.data, input_label_var.data)\n",
        "\n",
        "\n",
        "    print('%s Epoch: %d  , Loss: %.4f,  Accuracy: %.2f'%(logger_prefix, epoch, meter_loss.value()[0], acc.value()[0]))\n",
        "    return meter_loss.value()[0]\n",
        "\n",
        "num_epochs = 301\n",
        "evaluate_every_epoch = 5\n",
        "train_loss=[]\n",
        "validation_loss=[]\n",
        "for e in range(num_epochs):\n",
        "    loss=run_epoch(trLD, model, criterion, e, True, optimizer)\n",
        "    train_loss.append(loss)\n",
        "    if e % evaluate_every_epoch == 0:\n",
        "        val_loss=run_epoch(valLD, model, criterion, e, False, None)  \n",
        "        validation_loss.append(val_loss)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:30: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Epoch: 0  , Loss: 0.0521,  Accuracy: 98.35\n",
            "val Epoch: 0  , Loss: 1.0808,  Accuracy: 78.75\n",
            "train Epoch: 1  , Loss: 0.0836,  Accuracy: 97.10\n",
            "train Epoch: 2  , Loss: 0.0548,  Accuracy: 98.17\n",
            "train Epoch: 3  , Loss: 0.0665,  Accuracy: 97.82\n",
            "train Epoch: 4  , Loss: 0.0370,  Accuracy: 98.88\n",
            "train Epoch: 5  , Loss: 0.0523,  Accuracy: 98.58\n",
            "val Epoch: 5  , Loss: 1.1062,  Accuracy: 79.88\n",
            "train Epoch: 6  , Loss: 0.0627,  Accuracy: 98.20\n",
            "train Epoch: 7  , Loss: 0.0485,  Accuracy: 98.72\n",
            "train Epoch: 8  , Loss: 0.0451,  Accuracy: 98.58\n",
            "train Epoch: 9  , Loss: 0.0241,  Accuracy: 99.58\n",
            "train Epoch: 10  , Loss: 0.0199,  Accuracy: 99.72\n",
            "val Epoch: 10  , Loss: 1.0870,  Accuracy: 80.88\n",
            "train Epoch: 11  , Loss: 0.0177,  Accuracy: 99.85\n",
            "train Epoch: 12  , Loss: 0.0691,  Accuracy: 97.75\n",
            "train Epoch: 13  , Loss: 0.0401,  Accuracy: 98.80\n",
            "train Epoch: 14  , Loss: 0.2154,  Accuracy: 93.97\n",
            "train Epoch: 15  , Loss: 0.0542,  Accuracy: 98.50\n",
            "val Epoch: 15  , Loss: 1.1089,  Accuracy: 79.12\n",
            "train Epoch: 16  , Loss: 0.0316,  Accuracy: 99.33\n",
            "train Epoch: 17  , Loss: 0.0311,  Accuracy: 99.25\n",
            "train Epoch: 18  , Loss: 0.0359,  Accuracy: 99.10\n",
            "train Epoch: 19  , Loss: 0.0252,  Accuracy: 99.48\n",
            "train Epoch: 20  , Loss: 0.0457,  Accuracy: 98.55\n",
            "val Epoch: 20  , Loss: 1.1439,  Accuracy: 79.75\n",
            "train Epoch: 21  , Loss: 0.0745,  Accuracy: 97.70\n",
            "train Epoch: 22  , Loss: 0.1098,  Accuracy: 96.52\n",
            "train Epoch: 23  , Loss: 0.0386,  Accuracy: 98.90\n",
            "train Epoch: 24  , Loss: 0.0342,  Accuracy: 99.00\n",
            "train Epoch: 25  , Loss: 0.0241,  Accuracy: 99.55\n",
            "val Epoch: 25  , Loss: 1.1883,  Accuracy: 79.62\n",
            "train Epoch: 26  , Loss: 0.0246,  Accuracy: 99.35\n",
            "train Epoch: 27  , Loss: 0.0174,  Accuracy: 99.67\n",
            "train Epoch: 28  , Loss: 0.0452,  Accuracy: 98.70\n",
            "train Epoch: 29  , Loss: 0.0319,  Accuracy: 99.05\n",
            "train Epoch: 30  , Loss: 0.0249,  Accuracy: 99.42\n",
            "val Epoch: 30  , Loss: 1.1543,  Accuracy: 78.88\n",
            "train Epoch: 31  , Loss: 0.0289,  Accuracy: 99.17\n",
            "train Epoch: 32  , Loss: 0.0659,  Accuracy: 97.90\n",
            "train Epoch: 33  , Loss: 0.0464,  Accuracy: 98.62\n",
            "train Epoch: 34  , Loss: 0.0441,  Accuracy: 98.62\n",
            "train Epoch: 35  , Loss: 0.0469,  Accuracy: 98.60\n",
            "val Epoch: 35  , Loss: 1.5342,  Accuracy: 73.62\n",
            "train Epoch: 36  , Loss: 0.1112,  Accuracy: 96.58\n",
            "train Epoch: 37  , Loss: 0.0338,  Accuracy: 99.15\n",
            "train Epoch: 38  , Loss: 0.0212,  Accuracy: 99.60\n",
            "train Epoch: 39  , Loss: 0.0563,  Accuracy: 98.28\n",
            "train Epoch: 40  , Loss: 0.0324,  Accuracy: 99.08\n",
            "val Epoch: 40  , Loss: 1.1138,  Accuracy: 79.88\n",
            "train Epoch: 41  , Loss: 0.0230,  Accuracy: 99.60\n",
            "train Epoch: 42  , Loss: 0.0119,  Accuracy: 99.85\n",
            "train Epoch: 43  , Loss: 0.0144,  Accuracy: 99.80\n",
            "train Epoch: 44  , Loss: 0.0261,  Accuracy: 99.33\n",
            "train Epoch: 45  , Loss: 0.0415,  Accuracy: 98.65\n",
            "val Epoch: 45  , Loss: 1.1895,  Accuracy: 77.38\n",
            "train Epoch: 46  , Loss: 0.0812,  Accuracy: 97.45\n",
            "train Epoch: 47  , Loss: 0.0312,  Accuracy: 99.38\n",
            "train Epoch: 48  , Loss: 0.0208,  Accuracy: 99.55\n",
            "train Epoch: 49  , Loss: 0.0433,  Accuracy: 98.70\n",
            "train Epoch: 50  , Loss: 0.0345,  Accuracy: 99.08\n",
            "val Epoch: 50  , Loss: 1.1167,  Accuracy: 80.88\n",
            "train Epoch: 51  , Loss: 0.0319,  Accuracy: 99.10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train Epoch: 52  , Loss: 0.0516,  Accuracy: 98.40\n",
            "train Epoch: 53  , Loss: 0.0858,  Accuracy: 97.28\n",
            "train Epoch: 54  , Loss: 0.2269,  Accuracy: 92.58\n",
            "train Epoch: 55  , Loss: 0.0903,  Accuracy: 97.17\n",
            "val Epoch: 55  , Loss: 1.1485,  Accuracy: 78.38\n",
            "train Epoch: 56  , Loss: 0.0517,  Accuracy: 98.50\n",
            "train Epoch: 57  , Loss: 0.0420,  Accuracy: 99.10\n",
            "train Epoch: 58  , Loss: 0.1815,  Accuracy: 94.80\n",
            "train Epoch: 59  , Loss: 0.2017,  Accuracy: 94.27\n",
            "train Epoch: 60  , Loss: 0.1034,  Accuracy: 96.85\n",
            "val Epoch: 60  , Loss: 1.1344,  Accuracy: 76.88\n",
            "train Epoch: 61  , Loss: 0.1340,  Accuracy: 95.30\n",
            "train Epoch: 62  , Loss: 0.0420,  Accuracy: 98.85\n",
            "train Epoch: 63  , Loss: 0.0930,  Accuracy: 97.20\n",
            "train Epoch: 64  , Loss: 0.0347,  Accuracy: 99.02\n",
            "train Epoch: 65  , Loss: 0.0510,  Accuracy: 98.55\n",
            "val Epoch: 65  , Loss: 1.1007,  Accuracy: 78.00\n",
            "train Epoch: 66  , Loss: 0.0209,  Accuracy: 99.62\n",
            "train Epoch: 67  , Loss: 0.0270,  Accuracy: 99.28\n",
            "train Epoch: 68  , Loss: 0.0255,  Accuracy: 99.33\n",
            "train Epoch: 69  , Loss: 0.0140,  Accuracy: 99.72\n",
            "train Epoch: 70  , Loss: 0.0224,  Accuracy: 99.45\n",
            "val Epoch: 70  , Loss: 1.2253,  Accuracy: 77.75\n",
            "train Epoch: 71  , Loss: 0.0214,  Accuracy: 99.52\n",
            "train Epoch: 72  , Loss: 0.0230,  Accuracy: 99.40\n",
            "train Epoch: 73  , Loss: 0.0229,  Accuracy: 99.50\n",
            "train Epoch: 74  , Loss: 0.0128,  Accuracy: 99.85\n",
            "train Epoch: 75  , Loss: 0.0148,  Accuracy: 99.83\n",
            "val Epoch: 75  , Loss: 1.1935,  Accuracy: 78.75\n",
            "train Epoch: 76  , Loss: 0.0140,  Accuracy: 99.83\n",
            "train Epoch: 77  , Loss: 0.0111,  Accuracy: 99.85\n",
            "train Epoch: 78  , Loss: 0.0553,  Accuracy: 98.10\n",
            "train Epoch: 79  , Loss: 0.0726,  Accuracy: 97.70\n",
            "train Epoch: 80  , Loss: 0.0459,  Accuracy: 98.72\n",
            "val Epoch: 80  , Loss: 1.1252,  Accuracy: 78.50\n",
            "train Epoch: 81  , Loss: 0.0131,  Accuracy: 99.88\n",
            "train Epoch: 82  , Loss: 0.0096,  Accuracy: 99.92\n",
            "train Epoch: 83  , Loss: 0.0109,  Accuracy: 99.83\n",
            "train Epoch: 84  , Loss: 0.0226,  Accuracy: 99.30\n",
            "train Epoch: 85  , Loss: 0.0863,  Accuracy: 97.38\n",
            "val Epoch: 85  , Loss: 1.1834,  Accuracy: 77.25\n",
            "train Epoch: 86  , Loss: 0.2381,  Accuracy: 92.70\n",
            "train Epoch: 87  , Loss: 0.0657,  Accuracy: 97.82\n",
            "train Epoch: 88  , Loss: 0.0288,  Accuracy: 99.20\n",
            "train Epoch: 89  , Loss: 0.0207,  Accuracy: 99.55\n",
            "train Epoch: 90  , Loss: 0.0230,  Accuracy: 99.48\n",
            "val Epoch: 90  , Loss: 1.1176,  Accuracy: 79.25\n",
            "train Epoch: 91  , Loss: 0.0319,  Accuracy: 99.28\n",
            "train Epoch: 92  , Loss: 0.0223,  Accuracy: 99.58\n",
            "train Epoch: 93  , Loss: 0.0214,  Accuracy: 99.55\n",
            "train Epoch: 94  , Loss: 0.0198,  Accuracy: 99.52\n",
            "train Epoch: 95  , Loss: 0.0150,  Accuracy: 99.67\n",
            "val Epoch: 95  , Loss: 1.1339,  Accuracy: 79.75\n",
            "train Epoch: 96  , Loss: 0.0195,  Accuracy: 99.58\n",
            "train Epoch: 97  , Loss: 0.0166,  Accuracy: 99.55\n",
            "train Epoch: 98  , Loss: 0.0143,  Accuracy: 99.67\n",
            "train Epoch: 99  , Loss: 0.0133,  Accuracy: 99.70\n",
            "train Epoch: 100  , Loss: 0.0127,  Accuracy: 99.70\n",
            "val Epoch: 100  , Loss: 1.1433,  Accuracy: 80.12\n",
            "train Epoch: 101  , Loss: 0.0082,  Accuracy: 99.98\n",
            "train Epoch: 102  , Loss: 0.0453,  Accuracy: 98.70\n",
            "train Epoch: 103  , Loss: 0.0163,  Accuracy: 99.65\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train Epoch: 104  , Loss: 0.0154,  Accuracy: 99.70\n",
            "train Epoch: 105  , Loss: 0.0102,  Accuracy: 99.85\n",
            "val Epoch: 105  , Loss: 1.1451,  Accuracy: 79.62\n",
            "train Epoch: 106  , Loss: 0.0432,  Accuracy: 98.85\n",
            "train Epoch: 107  , Loss: 0.0185,  Accuracy: 99.55\n",
            "train Epoch: 108  , Loss: 0.0201,  Accuracy: 99.48\n",
            "train Epoch: 109  , Loss: 0.0151,  Accuracy: 99.62\n",
            "train Epoch: 110  , Loss: 0.0208,  Accuracy: 99.33\n",
            "val Epoch: 110  , Loss: 1.3168,  Accuracy: 77.12\n",
            "train Epoch: 111  , Loss: 0.0116,  Accuracy: 99.85\n",
            "train Epoch: 112  , Loss: 0.0134,  Accuracy: 99.75\n",
            "train Epoch: 113  , Loss: 0.0155,  Accuracy: 99.55\n",
            "train Epoch: 114  , Loss: 0.0324,  Accuracy: 99.00\n",
            "train Epoch: 115  , Loss: 0.0177,  Accuracy: 99.52\n",
            "val Epoch: 115  , Loss: 1.1610,  Accuracy: 79.12\n",
            "train Epoch: 116  , Loss: 0.0091,  Accuracy: 99.90\n",
            "train Epoch: 117  , Loss: 0.0096,  Accuracy: 99.88\n",
            "train Epoch: 118  , Loss: 0.0113,  Accuracy: 99.78\n",
            "train Epoch: 119  , Loss: 0.0181,  Accuracy: 99.58\n",
            "train Epoch: 120  , Loss: 0.0140,  Accuracy: 99.78\n",
            "val Epoch: 120  , Loss: 1.1577,  Accuracy: 78.62\n",
            "train Epoch: 121  , Loss: 0.0203,  Accuracy: 99.45\n",
            "train Epoch: 122  , Loss: 0.0099,  Accuracy: 99.83\n",
            "train Epoch: 123  , Loss: 0.0072,  Accuracy: 99.98\n",
            "train Epoch: 124  , Loss: 0.0225,  Accuracy: 99.40\n",
            "train Epoch: 125  , Loss: 0.0101,  Accuracy: 99.78\n",
            "val Epoch: 125  , Loss: 1.1995,  Accuracy: 80.12\n",
            "train Epoch: 126  , Loss: 0.0057,  Accuracy: 99.98\n",
            "train Epoch: 127  , Loss: 0.0059,  Accuracy: 99.95\n",
            "train Epoch: 128  , Loss: 0.0473,  Accuracy: 98.32\n",
            "train Epoch: 129  , Loss: 0.0623,  Accuracy: 98.02\n",
            "train Epoch: 130  , Loss: 0.0161,  Accuracy: 99.65\n",
            "val Epoch: 130  , Loss: 1.2161,  Accuracy: 79.50\n",
            "train Epoch: 131  , Loss: 0.0186,  Accuracy: 99.52\n",
            "train Epoch: 132  , Loss: 0.0110,  Accuracy: 99.78\n",
            "train Epoch: 133  , Loss: 0.0741,  Accuracy: 97.62\n",
            "train Epoch: 134  , Loss: 0.2013,  Accuracy: 93.62\n",
            "train Epoch: 135  , Loss: 0.1691,  Accuracy: 94.42\n",
            "val Epoch: 135  , Loss: 1.1927,  Accuracy: 79.12\n",
            "train Epoch: 136  , Loss: 0.0630,  Accuracy: 97.82\n",
            "train Epoch: 137  , Loss: 0.0927,  Accuracy: 97.05\n",
            "train Epoch: 138  , Loss: 0.0427,  Accuracy: 99.02\n",
            "train Epoch: 139  , Loss: 0.1423,  Accuracy: 95.55\n",
            "train Epoch: 140  , Loss: 0.0590,  Accuracy: 98.08\n",
            "val Epoch: 140  , Loss: 1.2023,  Accuracy: 79.00\n",
            "train Epoch: 141  , Loss: 0.1262,  Accuracy: 96.00\n",
            "train Epoch: 142  , Loss: 0.0161,  Accuracy: 99.78\n",
            "train Epoch: 143  , Loss: 0.0165,  Accuracy: 99.60\n",
            "train Epoch: 144  , Loss: 0.0145,  Accuracy: 99.83\n",
            "train Epoch: 145  , Loss: 0.0137,  Accuracy: 99.72\n",
            "val Epoch: 145  , Loss: 1.1431,  Accuracy: 79.00\n",
            "train Epoch: 146  , Loss: 0.0224,  Accuracy: 99.48\n",
            "train Epoch: 147  , Loss: 0.0207,  Accuracy: 99.45\n",
            "train Epoch: 148  , Loss: 0.0123,  Accuracy: 99.75\n",
            "train Epoch: 149  , Loss: 0.0138,  Accuracy: 99.70\n",
            "train Epoch: 150  , Loss: 0.0144,  Accuracy: 99.72\n",
            "val Epoch: 150  , Loss: 1.2374,  Accuracy: 79.50\n",
            "train Epoch: 151  , Loss: 0.0126,  Accuracy: 99.70\n",
            "train Epoch: 152  , Loss: 0.0082,  Accuracy: 99.95\n",
            "train Epoch: 153  , Loss: 0.0078,  Accuracy: 99.88\n",
            "train Epoch: 154  , Loss: 0.0115,  Accuracy: 99.70\n",
            "train Epoch: 155  , Loss: 0.0117,  Accuracy: 99.70\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "val Epoch: 155  , Loss: 1.2623,  Accuracy: 78.75\n",
            "train Epoch: 156  , Loss: 0.0165,  Accuracy: 99.45\n",
            "train Epoch: 157  , Loss: 0.0176,  Accuracy: 99.58\n",
            "train Epoch: 158  , Loss: 0.0102,  Accuracy: 99.83\n",
            "train Epoch: 159  , Loss: 0.0139,  Accuracy: 99.67\n",
            "train Epoch: 160  , Loss: 0.0418,  Accuracy: 98.90\n",
            "val Epoch: 160  , Loss: 1.2122,  Accuracy: 79.12\n",
            "train Epoch: 161  , Loss: 0.0113,  Accuracy: 99.75\n",
            "train Epoch: 162  , Loss: 0.0109,  Accuracy: 99.80\n",
            "train Epoch: 163  , Loss: 0.0575,  Accuracy: 98.58\n",
            "train Epoch: 164  , Loss: 0.0693,  Accuracy: 97.38\n",
            "train Epoch: 165  , Loss: 0.0146,  Accuracy: 99.60\n",
            "val Epoch: 165  , Loss: 1.1878,  Accuracy: 79.12\n",
            "train Epoch: 166  , Loss: 0.0246,  Accuracy: 99.25\n",
            "train Epoch: 167  , Loss: 0.0362,  Accuracy: 98.98\n",
            "train Epoch: 168  , Loss: 0.0160,  Accuracy: 99.65\n",
            "train Epoch: 169  , Loss: 0.0365,  Accuracy: 98.95\n",
            "train Epoch: 170  , Loss: 0.0168,  Accuracy: 99.62\n",
            "val Epoch: 170  , Loss: 1.1993,  Accuracy: 78.62\n",
            "train Epoch: 171  , Loss: 0.0358,  Accuracy: 98.88\n",
            "train Epoch: 172  , Loss: 0.0263,  Accuracy: 99.22\n",
            "train Epoch: 173  , Loss: 0.0209,  Accuracy: 99.58\n",
            "train Epoch: 174  , Loss: 0.0161,  Accuracy: 99.58\n",
            "train Epoch: 175  , Loss: 0.0089,  Accuracy: 99.85\n",
            "val Epoch: 175  , Loss: 1.1409,  Accuracy: 78.88\n",
            "train Epoch: 176  , Loss: 0.0082,  Accuracy: 99.83\n",
            "train Epoch: 177  , Loss: 0.0055,  Accuracy: 99.98\n",
            "train Epoch: 178  , Loss: 0.0070,  Accuracy: 99.88\n",
            "train Epoch: 179  , Loss: 0.0047,  Accuracy: 99.95\n",
            "train Epoch: 180  , Loss: 0.0044,  Accuracy: 100.00\n",
            "val Epoch: 180  , Loss: 1.1863,  Accuracy: 79.75\n",
            "train Epoch: 181  , Loss: 0.0043,  Accuracy: 99.98\n",
            "train Epoch: 182  , Loss: 0.0041,  Accuracy: 100.00\n",
            "train Epoch: 183  , Loss: 0.0037,  Accuracy: 100.00\n",
            "train Epoch: 184  , Loss: 0.0057,  Accuracy: 99.90\n",
            "train Epoch: 185  , Loss: 0.0095,  Accuracy: 99.83\n",
            "val Epoch: 185  , Loss: 1.1464,  Accuracy: 80.25\n",
            "train Epoch: 186  , Loss: 0.0043,  Accuracy: 99.98\n",
            "train Epoch: 187  , Loss: 0.0065,  Accuracy: 99.90\n",
            "train Epoch: 188  , Loss: 0.0053,  Accuracy: 99.92\n",
            "train Epoch: 189  , Loss: 0.0039,  Accuracy: 99.98\n",
            "train Epoch: 190  , Loss: 0.0037,  Accuracy: 99.95\n",
            "val Epoch: 190  , Loss: 1.2116,  Accuracy: 79.62\n",
            "train Epoch: 191  , Loss: 0.0051,  Accuracy: 99.95\n",
            "train Epoch: 192  , Loss: 0.0115,  Accuracy: 99.72\n",
            "train Epoch: 193  , Loss: 0.0085,  Accuracy: 99.88\n",
            "train Epoch: 194  , Loss: 0.1191,  Accuracy: 96.33\n",
            "train Epoch: 195  , Loss: 0.1080,  Accuracy: 96.92\n",
            "val Epoch: 195  , Loss: 1.2684,  Accuracy: 77.50\n",
            "train Epoch: 196  , Loss: 0.0375,  Accuracy: 98.83\n",
            "train Epoch: 197  , Loss: 0.0366,  Accuracy: 98.95\n",
            "train Epoch: 198  , Loss: 0.1881,  Accuracy: 94.25\n",
            "train Epoch: 199  , Loss: 0.0538,  Accuracy: 98.20\n",
            "train Epoch: 200  , Loss: 0.0279,  Accuracy: 99.22\n",
            "val Epoch: 200  , Loss: 1.2337,  Accuracy: 80.00\n",
            "train Epoch: 201  , Loss: 0.0144,  Accuracy: 99.70\n",
            "train Epoch: 202  , Loss: 0.0182,  Accuracy: 99.55\n",
            "train Epoch: 203  , Loss: 0.0212,  Accuracy: 99.40\n",
            "train Epoch: 204  , Loss: 0.0144,  Accuracy: 99.67\n",
            "train Epoch: 205  , Loss: 0.0111,  Accuracy: 99.75\n",
            "val Epoch: 205  , Loss: 1.1810,  Accuracy: 78.62\n",
            "train Epoch: 206  , Loss: 0.0072,  Accuracy: 99.98\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train Epoch: 207  , Loss: 0.0244,  Accuracy: 99.40\n",
            "train Epoch: 208  , Loss: 0.0131,  Accuracy: 99.70\n",
            "train Epoch: 209  , Loss: 0.0213,  Accuracy: 99.42\n",
            "train Epoch: 210  , Loss: 0.0175,  Accuracy: 99.50\n",
            "val Epoch: 210  , Loss: 1.3078,  Accuracy: 78.50\n",
            "train Epoch: 211  , Loss: 0.0135,  Accuracy: 99.67\n",
            "train Epoch: 212  , Loss: 0.0240,  Accuracy: 99.28\n",
            "train Epoch: 213  , Loss: 0.0176,  Accuracy: 99.62\n",
            "train Epoch: 214  , Loss: 0.0200,  Accuracy: 99.30\n",
            "train Epoch: 215  , Loss: 0.0083,  Accuracy: 99.88\n",
            "val Epoch: 215  , Loss: 1.2824,  Accuracy: 78.75\n",
            "train Epoch: 216  , Loss: 0.0063,  Accuracy: 99.90\n",
            "train Epoch: 217  , Loss: 0.0138,  Accuracy: 99.60\n",
            "train Epoch: 218  , Loss: 0.0360,  Accuracy: 99.10\n",
            "train Epoch: 219  , Loss: 0.0077,  Accuracy: 99.90\n",
            "train Epoch: 220  , Loss: 0.0080,  Accuracy: 99.92\n",
            "val Epoch: 220  , Loss: 1.2953,  Accuracy: 79.38\n",
            "train Epoch: 221  , Loss: 0.0086,  Accuracy: 99.88\n",
            "train Epoch: 222  , Loss: 0.0090,  Accuracy: 99.80\n",
            "train Epoch: 223  , Loss: 0.0056,  Accuracy: 99.98\n",
            "train Epoch: 224  , Loss: 0.0057,  Accuracy: 99.92\n",
            "train Epoch: 225  , Loss: 0.0351,  Accuracy: 99.17\n",
            "val Epoch: 225  , Loss: 1.3959,  Accuracy: 79.00\n",
            "train Epoch: 226  , Loss: 0.0338,  Accuracy: 99.05\n",
            "train Epoch: 227  , Loss: 0.0096,  Accuracy: 99.80\n",
            "train Epoch: 228  , Loss: 0.0158,  Accuracy: 99.67\n",
            "train Epoch: 229  , Loss: 0.1435,  Accuracy: 95.80\n",
            "train Epoch: 230  , Loss: 0.0957,  Accuracy: 97.05\n",
            "val Epoch: 230  , Loss: 1.2220,  Accuracy: 78.38\n",
            "train Epoch: 231  , Loss: 0.2088,  Accuracy: 94.03\n",
            "train Epoch: 232  , Loss: 0.0598,  Accuracy: 98.30\n",
            "train Epoch: 233  , Loss: 0.0159,  Accuracy: 99.70\n",
            "train Epoch: 234  , Loss: 0.0135,  Accuracy: 99.70\n",
            "train Epoch: 235  , Loss: 0.0156,  Accuracy: 99.70\n",
            "val Epoch: 235  , Loss: 1.2289,  Accuracy: 79.12\n",
            "train Epoch: 236  , Loss: 0.0083,  Accuracy: 99.92\n",
            "train Epoch: 237  , Loss: 0.1750,  Accuracy: 95.05\n",
            "train Epoch: 238  , Loss: 0.1244,  Accuracy: 95.83\n",
            "train Epoch: 239  , Loss: 0.0585,  Accuracy: 98.05\n",
            "train Epoch: 240  , Loss: 0.0630,  Accuracy: 97.88\n",
            "val Epoch: 240  , Loss: 1.1811,  Accuracy: 79.12\n",
            "train Epoch: 241  , Loss: 0.1896,  Accuracy: 94.38\n",
            "train Epoch: 242  , Loss: 0.0984,  Accuracy: 97.12\n",
            "train Epoch: 243  , Loss: 0.0719,  Accuracy: 97.82\n",
            "train Epoch: 244  , Loss: 0.0269,  Accuracy: 99.17\n",
            "train Epoch: 245  , Loss: 0.0283,  Accuracy: 99.17\n",
            "val Epoch: 245  , Loss: 1.1969,  Accuracy: 79.75\n",
            "train Epoch: 246  , Loss: 0.1421,  Accuracy: 96.15\n",
            "train Epoch: 247  , Loss: 0.0325,  Accuracy: 98.88\n",
            "train Epoch: 248  , Loss: 0.0363,  Accuracy: 99.00\n",
            "train Epoch: 249  , Loss: 0.0288,  Accuracy: 99.10\n",
            "train Epoch: 250  , Loss: 0.0170,  Accuracy: 99.52\n",
            "val Epoch: 250  , Loss: 1.2384,  Accuracy: 78.50\n",
            "train Epoch: 251  , Loss: 0.0404,  Accuracy: 98.90\n",
            "train Epoch: 252  , Loss: 0.0157,  Accuracy: 99.67\n",
            "train Epoch: 253  , Loss: 0.0110,  Accuracy: 99.70\n",
            "train Epoch: 254  , Loss: 0.0181,  Accuracy: 99.48\n",
            "train Epoch: 255  , Loss: 0.0199,  Accuracy: 99.40\n",
            "val Epoch: 255  , Loss: 1.2542,  Accuracy: 79.00\n",
            "train Epoch: 256  , Loss: 0.0138,  Accuracy: 99.72\n",
            "train Epoch: 257  , Loss: 0.0081,  Accuracy: 99.92\n",
            "train Epoch: 258  , Loss: 0.0066,  Accuracy: 99.92\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train Epoch: 259  , Loss: 0.0134,  Accuracy: 99.67\n",
            "train Epoch: 260  , Loss: 0.0259,  Accuracy: 99.35\n",
            "val Epoch: 260  , Loss: 1.2178,  Accuracy: 80.75\n",
            "train Epoch: 261  , Loss: 0.0118,  Accuracy: 99.75\n",
            "train Epoch: 262  , Loss: 0.0130,  Accuracy: 99.72\n",
            "train Epoch: 263  , Loss: 0.0105,  Accuracy: 99.78\n",
            "train Epoch: 264  , Loss: 0.0131,  Accuracy: 99.80\n",
            "train Epoch: 265  , Loss: 0.0064,  Accuracy: 99.92\n",
            "val Epoch: 265  , Loss: 1.2354,  Accuracy: 79.50\n",
            "train Epoch: 266  , Loss: 0.0059,  Accuracy: 99.90\n",
            "train Epoch: 267  , Loss: 0.0167,  Accuracy: 99.58\n",
            "train Epoch: 268  , Loss: 0.0088,  Accuracy: 99.90\n",
            "train Epoch: 269  , Loss: 0.0084,  Accuracy: 99.80\n",
            "train Epoch: 270  , Loss: 0.0063,  Accuracy: 99.90\n",
            "val Epoch: 270  , Loss: 1.2645,  Accuracy: 79.25\n",
            "train Epoch: 271  , Loss: 0.0072,  Accuracy: 99.88\n",
            "train Epoch: 272  , Loss: 0.0046,  Accuracy: 99.95\n",
            "train Epoch: 273  , Loss: 0.0054,  Accuracy: 99.98\n",
            "train Epoch: 274  , Loss: 0.0125,  Accuracy: 99.55\n",
            "train Epoch: 275  , Loss: 0.0137,  Accuracy: 99.60\n",
            "val Epoch: 275  , Loss: 1.2498,  Accuracy: 79.25\n",
            "train Epoch: 276  , Loss: 0.0094,  Accuracy: 99.78\n",
            "train Epoch: 277  , Loss: 0.0067,  Accuracy: 99.92\n",
            "train Epoch: 278  , Loss: 0.0063,  Accuracy: 99.90\n",
            "train Epoch: 279  , Loss: 0.0057,  Accuracy: 99.88\n",
            "train Epoch: 280  , Loss: 0.0057,  Accuracy: 99.95\n",
            "val Epoch: 280  , Loss: 1.2478,  Accuracy: 79.50\n",
            "train Epoch: 281  , Loss: 0.0532,  Accuracy: 98.42\n",
            "train Epoch: 282  , Loss: 0.0461,  Accuracy: 98.45\n",
            "train Epoch: 283  , Loss: 0.0454,  Accuracy: 98.80\n",
            "train Epoch: 284  , Loss: 0.0212,  Accuracy: 99.50\n",
            "train Epoch: 285  , Loss: 0.0120,  Accuracy: 99.70\n",
            "val Epoch: 285  , Loss: 1.2956,  Accuracy: 79.25\n",
            "train Epoch: 286  , Loss: 0.0082,  Accuracy: 99.85\n",
            "train Epoch: 287  , Loss: 0.0056,  Accuracy: 99.95\n",
            "train Epoch: 288  , Loss: 0.0084,  Accuracy: 99.80\n",
            "train Epoch: 289  , Loss: 0.0093,  Accuracy: 99.78\n",
            "train Epoch: 290  , Loss: 0.0090,  Accuracy: 99.78\n",
            "val Epoch: 290  , Loss: 1.2833,  Accuracy: 78.50\n",
            "train Epoch: 291  , Loss: 0.0048,  Accuracy: 99.98\n",
            "train Epoch: 292  , Loss: 0.0185,  Accuracy: 99.45\n",
            "train Epoch: 293  , Loss: 0.0668,  Accuracy: 98.42\n",
            "train Epoch: 294  , Loss: 0.0142,  Accuracy: 99.62\n",
            "train Epoch: 295  , Loss: 0.0249,  Accuracy: 99.35\n",
            "val Epoch: 295  , Loss: 1.3066,  Accuracy: 79.12\n",
            "train Epoch: 296  , Loss: 0.0109,  Accuracy: 99.70\n",
            "train Epoch: 297  , Loss: 0.0067,  Accuracy: 99.83\n",
            "train Epoch: 298  , Loss: 0.0059,  Accuracy: 99.85\n",
            "train Epoch: 299  , Loss: 0.0075,  Accuracy: 99.88\n",
            "train Epoch: 300  , Loss: 0.0048,  Accuracy: 99.95\n",
            "val Epoch: 300  , Loss: 1.3237,  Accuracy: 79.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z8-IvsVaedLD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_temp = np.arange(1,302)\n",
        "plt.xlim(0,1)\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Train Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.plot(train_temp,train_loss)\n",
        "plt.show()\n",
        "\n",
        "val_temp = np.arange(1,62)\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Validation Loss')\n",
        "plt.title('Validation Loss')\n",
        "plt.plot(val_temp,validation_loss)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "igW9ytAO9x-R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Submit your results on Kaggle\n",
        "\n",
        "### Train a better model for action recognition!\n",
        "Now it's your job to experiment with architectures, hyperparameters, loss functions, and optimizers to train a model that achieves better accuracy on the action recognition validation set.\n",
        "\n",
        "\n",
        "### Testing the model and submit on Kaggle\n",
        "Testing the model on the testing set and save the results as a .csv file. \n",
        "Please submitted the results.csv file generated by predict_on_test() to Kaggle(https://www.kaggle.com/c/cse512springhw5) to see how well your network performs on the test set. \n",
        "################ 3rd To Do  (30 points, the highest 3 entries get extra 10 points) ###############\n"
      ]
    },
    {
      "metadata": {
        "id": "KiEDhHQl9x-S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2069abb8-bbe6-44e9-ba74-4a724ccf2c38",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525577382342,
          "user_tz": 240,
          "elapsed": 1345,
          "user": {
            "displayName": "Tejas Naik",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "109987961230950503945"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Use your best model to generate results on test set.\n",
        "\n",
        "# generate csv file for test set\n",
        "def predict_on_test(model, data_loader):\n",
        "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
        "    results=open('results_tejas_new_6.csv','w')\n",
        "    count=0\n",
        "    results.write('Id'+','+'Class'+'\\n')\n",
        "    for batch_idx, sample in enumerate(data_loader):\n",
        "        sequence = sample['seq']\n",
        "        input_sequence_var = Variable(sequence) \n",
        "        scores = model(input_sequence_var)\n",
        "        _, preds = scores.data.max(1)\n",
        "        for i in range(len(preds)):\n",
        "            x= str(preds[i])\n",
        "            ans=int(x[7])\n",
        "            results.write(str(count)+','+str(ans)+'\\n')\n",
        "            \n",
        "            #print ans\n",
        "            count+=1\n",
        "    results.close()\n",
        "    return count\n",
        "\n",
        "count=predict_on_test(model, tstLD)\n",
        "print(count)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5li8WdGpYmcS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('results_tejas_new_6.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8BCsjP_39x-V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Report the performance\n",
        "################ 4th To Do  (15 points)##################\n",
        "\n",
        "### Documentation of what you did\n",
        "In this cell, you should write an explanation of what you did (network architecture, optimiziter, learning rate, epoches) and visualizations or graphs of loss/accuracy curve tin the process of training and evaluating.\n",
        "\n",
        "I tried different architectures and played with different parameters and tuned it to get the best accuracy.\n",
        "My best architecture is as:\n",
        "\n",
        "1) Single LSTM layer with input size of 75, hidden size 120 and with dropout of 0.3\n",
        "2) ReLU Activation Layer\n",
        "3) Linear Layer\n",
        "4) ReLU Activation Layer\n",
        "5) Linear Layer\n",
        "\n",
        "I tried various optimizers like Adam, SGD and found SGD to be the best. I experimented by increasing and decreasing the learning rate and found e-1 to be the best learning rate.\n",
        "\n",
        "I experimented with 100, 200 ,300 , 400 and 500 epochs. 300 epochs gave me the best possible accuracy and after 300 the model starts overfitting.\n",
        "\n",
        "\n",
        "\n",
        "### performance on Kaggle\n",
        "You should also report your Kaggle Performance here: \n",
        "\n",
        "0.794"
      ]
    },
    {
      "metadata": {
        "id": "jR89h46B9x-V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}